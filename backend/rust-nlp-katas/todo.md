# Rust NLP Katas

## Phase 0 — Language & Text (Foundations)
- [x] Explore ambiguity in sentences (same word, different meanings)
- [x] Identify noise in real-world text samples
- [x] Compare structured vs unstructured text

## Phase 1 — Text Preprocessing
- [x] Apply preprocessing pipeline to raw text
- [x] Compare stemming vs lemmatization output
- [x] Measure how stopword removal changes document similarity

## Phase 2 — Bag of Words (BoW)
- [x] Build BoW from scratch
- [x] Visualize document vectors
- [x] Compare documents using BoW

## Phase 3 — TF-IDF
- [x] Compute TF-IDF manually
- [x] Compare similarity using BoW vs TF-IDF
- [x] Visualize word importance

## Phase 4 — Similarity & Classical NLP Tasks
- [x] Compute cosine similarity between document pairs
- [x] Build a simple text search engine
- [x] Cluster documents by topic

## Phase 5 — Tokenization (Deep Dive)
- [x] Tokenize text using word, character, and subword methods
- [x] Implement BPE from scratch
- [x] Compare vocabulary sizes and OOV handling across methods

## Phase 6 — Named Entity Recognition (NER)
- [x] Rule-based NER
- [x] Simple ML-based NER
- [x] Error analysis

## Phase 7 — Small Neural Text Models
- [x] Train small embedding-based models
- [x] Visualize embedding spaces
- [x] Compare neural vs TF-IDF models

## Phase 8 — Context & Sequence Modeling
- [x] Show how word order changes meaning (BoW failure cases)
- [x] Compare context-aware vs context-free representations
- [x] Demonstrate sequence modeling challenges (conceptual)

## Phase 9 — Transformer Architecture (Core Concepts)
- [x] Visualize attention weights
- [x] Build a tiny transformer block
- [x] Compare encoder-only vs decoder-only tasks

## Phase 10 — Modern NLP Pipelines (Awareness)
- [x] Pretraining vs fine-tuning overview
- [x] Encoder-only vs decoder-only models comparison
- [x] Where LLMs fit in the NLP stack
