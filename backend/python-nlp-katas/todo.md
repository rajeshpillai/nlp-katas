# Python NLP Katas

## Phase 0 — Language & Text (Foundations)
- [x] Explore ambiguity in sentences (same word, different meanings)
- [x] Identify noise in real-world text samples
- [x] Compare structured vs unstructured text

## Phase 1 — Text Preprocessing
- [x] Apply preprocessing pipeline to raw text
- [x] Compare stemming vs lemmatization output
- [x] Measure how stopword removal changes document similarity

## Phase 2 — Bag of Words (BoW)
- [ ] Build BoW from scratch
- [ ] Visualize document vectors
- [ ] Compare documents using BoW

## Phase 3 — TF-IDF
- [ ] Compute TF-IDF manually
- [ ] Compare similarity using BoW vs TF-IDF
- [ ] Visualize word importance

## Phase 4 — Similarity & Classical NLP Tasks
- [ ] Compute cosine similarity between document pairs
- [ ] Build a simple text search engine
- [ ] Cluster documents by topic

## Phase 5 — Tokenization (Deep Dive)
- [ ] Tokenize text using word, character, and subword methods
- [ ] Implement BPE from scratch
- [ ] Compare vocabulary sizes and OOV handling across methods

## Phase 6 — Named Entity Recognition (NER)
- [ ] Rule-based NER
- [ ] Simple ML-based NER
- [ ] Error analysis

## Phase 7 — Small Neural Text Models
- [ ] Train small embedding-based models
- [ ] Visualize embedding spaces
- [ ] Compare neural vs TF-IDF models

## Phase 8 — Context & Sequence Modeling
- [ ] Show how word order changes meaning (BoW failure cases)
- [ ] Compare context-aware vs context-free representations
- [ ] Demonstrate vanishing gradient problem in sequences (conceptual)

## Phase 9 — Transformer Architecture (Core Concepts)
- [ ] Visualize attention weights
- [ ] Build a tiny transformer block
- [ ] Compare encoder-only vs decoder-only tasks

## Phase 10 — Modern NLP Pipelines (Awareness)
- [ ] Pretraining vs fine-tuning overview
- [ ] Encoder-only vs decoder-only models comparison
- [ ] Where LLMs fit in the NLP stack
